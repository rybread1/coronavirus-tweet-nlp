{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coronavirus-tweet-nlp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOpIJmSUvUu9q1nhky3fx5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rybread1/coronavirus-tweet-nlp/blob/master/coronavirus_tweet_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KM9EjsHZ7jX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "865d4470-2820-40d1-ac82-672c30a08e65"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\r\u001b[K     |▍                               | 10kB 27.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 4.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 757kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 788kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 819kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 849kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 880kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 25.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=dafc0e48c2d55aac51348f167c8bb27b0b4f8ec3f005338b6c3b647fdf388d7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h78UkLjkaBbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "835056bd-afce-4630-8e22-c39a76c3d3e4"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn import metrics as sk_metrics\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqrLQyTUaCRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d05acdc-f8fb-45a7-db07-187077742224"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ddrG6EaDON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = '/content/gdrive/My Drive/corona-tweet-sent'\n",
        "train_f = 'Corona_NLP_train.csv'\n",
        "test_f = 'Corona_NLP_test.csv'\n",
        "\n",
        "df_train = pd.read_csv(os.path.join(folder, train_f), engine='python')\n",
        "df_test = pd.read_csv(os.path.join(folder, test_f), engine='python')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pklk6KusbGf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "ca799c08-31b3-49cd-c87d-dc4e33afa1ec"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41157 entries, 0 to 41156\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   UserName       41157 non-null  int64 \n",
            " 1   ScreenName     41157 non-null  int64 \n",
            " 2   Location       32567 non-null  object\n",
            " 3   TweetAt        41157 non-null  object\n",
            " 4   OriginalTweet  41157 non-null  object\n",
            " 5   Sentiment      41157 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 1.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhF9mMVKbnBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e84b1a0e-302e-4855-bf81-713dff5b76c0"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserName</th>\n",
              "      <th>ScreenName</th>\n",
              "      <th>Location</th>\n",
              "      <th>TweetAt</th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3799</td>\n",
              "      <td>48751</td>\n",
              "      <td>London</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3800</td>\n",
              "      <td>48752</td>\n",
              "      <td>UK</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>advice Talk to your neighbours family to excha...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3801</td>\n",
              "      <td>48753</td>\n",
              "      <td>Vagabonds</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3802</td>\n",
              "      <td>48754</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>My food stock is not the only one which is emp...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3803</td>\n",
              "      <td>48755</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
              "      <td>Extremely Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserName  ...           Sentiment\n",
              "0      3799  ...             Neutral\n",
              "1      3800  ...            Positive\n",
              "2      3801  ...            Positive\n",
              "3      3802  ...            Positive\n",
              "4      3803  ...  Extremely Negative\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbVsoiYsbuYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_map = {\n",
        "    'Positive': 0, \n",
        "    'Negative': 1, \n",
        "    'Neutral': 2, \n",
        "    'Extremely Positive': 0, \n",
        "    'Extremely Negative': 1\n",
        "}\n",
        "\n",
        "df_train['Sentiment_Cat'] = df_train['Sentiment'].map(y_map)\n",
        "df_test['Sentiment_Cat'] = df_test['Sentiment'].map(y_map)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mUdhBwNbwmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3887eae7-8d66-4e8c-e538-3fb2ccba5fd5"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def clean_text(data):\n",
        "    data = data.lower()\n",
        "    data = re.sub(r'([^a-zA-Z\\s])', '', data)\n",
        "    data = data.split()\n",
        "    temp = []\n",
        "    for i in data:\n",
        "        if i not in stop_words:\n",
        "            temp.append(i)\n",
        "    data = ' '.join(temp)\n",
        "    return data\n",
        "\n",
        "df_train['tweet_cleaned'] = df_train['OriginalTweet'].apply(clean_text)\n",
        "df_test['tweet_cleaned'] = df_test['OriginalTweet'].apply(clean_text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzhyEzc2dIsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = df_train['tweet_cleaned'].values, np.array(df_train['Sentiment_Cat'], dtype=np.int64)\n",
        "x_test, y_test = df_test['tweet_cleaned'].values, np.array(df_test['Sentiment_Cat'], dtype=np.int64)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfVEmb5Idiv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "import transformers"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o7uh0Gbdlmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "932d39c8-4e83-4d08-b31d-e80535ec7eb1"
      },
      "source": [
        "# First load the real tokenizer\n",
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower=True)\n",
        "# Save the loaded tokenizer locally\n",
        "tokenizer.save_pretrained('.')\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\n",
        "fast_tokenizer"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvy3OyJddodD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=200):\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(length=maxlen)\n",
        "    all_ids = []\n",
        "    \n",
        "    for i in range(0, len(texts), chunk_size):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "    \n",
        "    return np.array(all_ids)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9LJvgi0d5i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len=50\n",
        "\n",
        "x_train_enc = fast_encode(x_train, fast_tokenizer, maxlen=max_len)\n",
        "x_test_enc = fast_encode(x_test, fast_tokenizer, maxlen=max_len)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcBnfPChd9PO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "a00ea9d7-3dbc-4d5b-c2b8-17307b7924be"
      },
      "source": [
        "NUM_CLASSES = df_train['Sentiment_Cat'].nunique()\n",
        "\n",
        "bert_transformer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "input = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "sequence_output = bert_transformer(input)[0]\n",
        "cls_token = sequence_output[:, 0, :]\n",
        "output = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(cls_token)\n",
        "\n",
        "model = tf.keras.Model(inputs=input, outputs=output)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=7e-6), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bliWbCjeRLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "251aa25e-3375-4e06-8efb-d1b052262d40"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "tf_distil_bert_model_2 (TFDi ((None, 50, 768),)        66362880  \n",
            "_________________________________________________________________\n",
            "tf_op_layer_strided_slice_2  [(None, 768)]             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 2307      \n",
            "=================================================================\n",
            "Total params: 66,365,187\n",
            "Trainable params: 66,365,187\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJRsiDYFevgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "fbe2ba7d-8012-45d1-fa61-04a25c11133b"
      },
      "source": [
        "# call backs\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    patience=5, \n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_enc,\n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test_enc, y_test),\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop_callback])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "644/644 [==============================] - 224s 348ms/step - loss: 0.6677 - accuracy: 0.7190 - val_loss: 0.5366 - val_accuracy: 0.8002\n",
            "Epoch 2/50\n",
            "644/644 [==============================] - 222s 345ms/step - loss: 0.4653 - accuracy: 0.8319 - val_loss: 0.4923 - val_accuracy: 0.8189\n",
            "Epoch 3/50\n",
            "644/644 [==============================] - 222s 344ms/step - loss: 0.3937 - accuracy: 0.8618 - val_loss: 0.4566 - val_accuracy: 0.8418\n",
            "Epoch 4/50\n",
            "644/644 [==============================] - 222s 344ms/step - loss: 0.3411 - accuracy: 0.8848 - val_loss: 0.4546 - val_accuracy: 0.8399\n",
            "Epoch 5/50\n",
            "644/644 [==============================] - 222s 344ms/step - loss: 0.3030 - accuracy: 0.8990 - val_loss: 0.4593 - val_accuracy: 0.8481\n",
            "Epoch 6/50\n",
            "644/644 [==============================] - 222s 345ms/step - loss: 0.2666 - accuracy: 0.9129 - val_loss: 0.4489 - val_accuracy: 0.8507\n",
            "Epoch 7/50\n",
            "644/644 [==============================] - 222s 344ms/step - loss: 0.2362 - accuracy: 0.9224 - val_loss: 0.4644 - val_accuracy: 0.8481\n",
            "Epoch 8/50\n",
            "644/644 [==============================] - 221s 344ms/step - loss: 0.2024 - accuracy: 0.9328 - val_loss: 0.5001 - val_accuracy: 0.8499\n",
            "Epoch 9/50\n",
            "644/644 [==============================] - 222s 345ms/step - loss: 0.1786 - accuracy: 0.9405 - val_loss: 0.5404 - val_accuracy: 0.8454\n",
            "Epoch 10/50\n",
            "644/644 [==============================] - 222s 344ms/step - loss: 0.1535 - accuracy: 0.9497 - val_loss: 0.5541 - val_accuracy: 0.8402\n",
            "Epoch 11/50\n",
            "644/644 [==============================] - 222s 345ms/step - loss: 0.1341 - accuracy: 0.9569 - val_loss: 0.5698 - val_accuracy: 0.8412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9uNgVAe0MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}